# -*- coding: utf-8 -*-
"""asm-last.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SybmjGXGsP4ysOLKUsUg3yjyyefO_opH
"""

import pandas as pd

# Load the dataset
file_path = '/content/drive/MyDrive/asm-last-folder/asm.csv'
wine_data = pd.read_csv(file_path)

# Display the first few rows of the dataframe and a summary of the data
wine_data.head(), wine_data.info(), wine_data.describe(include='all')

import matplotlib.pyplot as plt
import seaborn as sns

# Set the aesthetic style of the plots
sns.set_style("whitegrid")

# Create a figure to hold multiple plots
fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(14, 12))

# Plot for distribution of points
sns.histplot(wine_data['points'], kde=True, ax=axes[0, 0])
axes[0, 0].set_title(' Points Distribution')

# Bar plot for superior_rating counts
sns.countplot(x='superior_rating', data=wine_data, ax=axes[1, 0])
axes[1, 0].set_title('Superior Rating Count')

# Plot distribution of price
sns.histplot(wine_data['price'], kde=True, ax=axes[0, 1])
axes[0, 1].set_title('Price Distribution')

# Bar plot for variety counts (top 20)
top_varieties = wine_data['variety'].value_counts().head(20)
sns.barplot(x=top_varieties.values, y=top_varieties.index, ax=axes[1, 1],palette='Blues_r')
axes[1, 1].set_title('Top 20 Wine Varieties')

# Adjust layout
plt.tight_layout()
plt.show()

import seaborn as sns
# Create a figure to hold multiple plots
fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(10, 16))

# Scatter plot of points vs price
#sns.scatterplot(data=wine_data, x='price', y='points', style='superior_rating', markers='x', ax=axes[0])

sns.scatterplot(data=wine_data, x='price', y='points', hue='superior_rating', style='superior_rating', markers={1: 'x', 0: '+'}, palette={1: 'red', 0: 'blue'}, ax=axes[0])
axes[0].set_title('Points vs Price')
axes[0].set_xlabel('Price ($)')
axes[0].set_ylabel('Points')
axes[0].legend(title='Superior Rating')

# Boxplot of points across different wine varieties (top 20 varieties)
top_varieties_list = wine_data['variety'].value_counts().head(20).index
subset_variety_data = wine_data[wine_data['variety'].isin(top_varieties_list)]
sns.boxplot(data=subset_variety_data, y='variety', x='points', ax=axes[1], palette='tab20')
axes[1].set_title('Points Distribution Across Top 20 Wine Varieties')
axes[1].set_xlabel('Points')
axes[1].set_ylabel('Variety')

# Adjust layout
plt.tight_layout()
plt.show()

# Compute the correlation matrix for numeric and binary indicator variables
correlation_matrix = wine_data.select_dtypes(include=['int64']).corr()

# Generate a heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap='viridis', cbar_kws={'label': 'Correlation coefficient'})
plt.title('Correlation Matrix of Wine Data Variables')
plt.show()

import plotly.express as px
import pandas as pd

# Assuming data is loaded into a DataFrame named 'data'
sunburst_data = wine_data.groupby(['variety', 'superior_rating']).size().reset_index(name='Count')

# Create the sunburst plot
fig = px.sunburst(sunburst_data, path=['variety', 'superior_rating'], values='Count',
                  color='superior_rating', color_continuous_scale=px.colors.sequential.Blues, title='Distribution of Wine Ratings by Variety')
fig.show()

import plotly.express as px
import pandas as pd

# Load the data (assuming it's already loaded as `data`)
# Normalize or scale the data if necessary (especially for 'points' and 'price')

# Example of how to normalize 'points' and 'price' if not yet normalized:
# from sklearn.preprocessing import MinMaxScaler
# scaler = MinMaxScaler()
# data[['points', 'price']] = scaler.fit_transform(data[['points', 'price']])

# Create a parallel coordinates plot
fig = px.parallel_coordinates(wine_data,
                              dimensions=['points', 'price', 'Crisp', 'Dry', 'Finish', 'Firm', 'Fresh', 'Fruit', 'Full', 'Rich', 'Round', 'Soft', 'Sweet'],
                              color="price",
                              labels={"points": "Points", "price": "Price", "Crisp": "Crisp", "Dry": "Dry",
                                      "Finish": "Finish", "Firm": "Firm", "Fresh": "Fresh", "Fruit": "Fruit",
                                      "Full": "Full", "Rich": "Rich", "Round": "Round", "Soft": "Soft", "Sweet": "Sweet"},
                              color_continuous_scale=px.colors.diverging.Tealrose,
                              title="Parallel Coordinates Plot for Wine Attributes")
# Show the plot
fig.show()

"""model stuff"""

import pandas as pd

# Load the dataset
file_path = '/content/drive/MyDrive/asm-last-folder/asm.csv'
wine_data = pd.read_csv(file_path)

from sklearn.preprocessing import LabelEncoder

# Encode the 'variety' categorical variable
encoder = LabelEncoder()
wine_data['variety_encoded'] = encoder.fit_transform(wine_data['variety'])

import pymc as pm
import numpy as np
import arviz as az

# Assume 'data' is a pandas DataFrame with our prepared wine data
X = wine_data[['price', 'variety_encoded']]  # and other relevant features
y = wine_data['superior_rating']

with pm.Model() as logistic_model:
    # Priors for unknown model parameters
    alpha = pm.Normal('alpha', mu=0, sigma=10)
    betas = pm.Normal('betas', mu=0, sigma=10, shape=(len(X.columns),))

    # Logistic regression equation
    logits = alpha + pm.math.dot(X, betas)

    # Likelihood (sampling distribution) of observations
    observed = pm.Bernoulli('observed', logit_p=logits, observed=y)

    # Sampling from the model
    trace = pm.sample(500, return_inferencedata=True)

# Plotting the posterior distributions of the model parameters using ArviZ
az.plot_posterior(trace)

import pymc as pm
import arviz as az
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score

# Selecting predictors and the target variable
X = wine_data[['points', 'price']]
y = wine_data['superior_rating']

# Standardizing the predictors
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

# Simplified model setup in PyMC with reduced complexity
with pm.Model() as logistic_model_reduced:
    # Priors for the coefficients
    intercept = pm.Normal("intercept", mu=0, sigma=10)
    beta_points = pm.Normal("beta_points", mu=0, sigma=10)
    beta_price = pm.Normal("beta_price", mu=0, sigma=10)
    beta_interaction = pm.Normal("beta_interaction", mu=0, sigma=10)

    # Linear model without interaction term for simplicity
    logits = intercept + beta_points * X_train[:, 0] + beta_price * X_train[:, 1]

    # Likelihood (Bernoulli distributed observations)
    observed = pm.Bernoulli("observed", logit_p=logits, observed=y_train)

    # MCMC settings: running 2 chains with reduced number of samples for quicker execution
    trace_reduced = pm.sample(1000, tune=1000, chains=4, return_inferencedata=True)

# Diagnostics and posterior plots using ArviZ
az.plot_trace(trace_reduced)
summary = az.summary(trace_reduced)
summary

az.plot_posterior(trace)